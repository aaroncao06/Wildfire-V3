{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# imports","metadata":{"id":"kDQmb5QD9nPW"}},{"cell_type":"code","source":"!pip install einops\n!pip install zarr\n!pip install xarray[io]\n!pip install -Uqq ipdb\nfrom numpy import save, load\nfrom pathlib import Path\nimport dask.array as da\nimport warnings\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nimport ipdb\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score, precision_score, accuracy_score, recall_score, roc_auc_score, average_precision_score\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nimport math\nfrom torch.optim.optimizer import Optimizer\nimport pandas as pd\nfrom einops import rearrange\nfrom torch.nn import functional as F\nimport xarray as xr\nfrom torch.utils.data import Dataset, DataLoader\nimport zarr\nimport sys\n","metadata":{"id":"8GGVQ6bM9sW_","outputId":"755c6265-4f42-4bdb-b3af-97933508ef1a","execution":{"iopub.status.busy":"2023-08-09T19:36:20.390657Z","iopub.execute_input":"2023-08-09T19:36:20.391039Z","iopub.status.idle":"2023-08-09T19:36:37.377163Z","shell.execute_reply.started":"2023-08-09T19:36:20.391011Z","shell.execute_reply":"2023-08-09T19:36:37.375552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pdb off","metadata":{"id":"IexRBIE4w8Mt","outputId":"98c38678-7b06-45c7-e519-63b1c0ed603e","execution":{"iopub.status.busy":"2023-08-09T19:36:37.377830Z","iopub.status.idle":"2023-08-09T19:36:37.378146Z","shell.execute_reply.started":"2023-08-09T19:36:37.377996Z","shell.execute_reply":"2023-08-09T19:36:37.378012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset + Dataloader","metadata":{"id":"YYXjKLYF8o5G"}},{"cell_type":"code","source":"class TemporalDatasetFromNumpy(Dataset):\n    def __init__(self, numpy_array, sequence_length, train_ratio, pin_memory):\n        self.numpy_array = numpy_array #shape (40, 40, 918, 29), lat, lon, time, var\n        self.sequence_length = sequence_length\n        self.pin_memory = pin_memory\n        \n        self.lat_size, self.lon_size, self.time_size, self.num_variables = self.numpy_array.shape\n\n        self.time_size -= (sequence_length) #num of timesteps with targets\n        self.numpy_array = self.numpy_array[:self.time_size]\n        # 40, 40, 854, 29\n        # 0 to 39\n        \n        self.train_ratio = train_ratio\n        \n        self.pos_indices = np.column_stack(np.where(self.numpy_array[:,:,self.sequence_length:, -1] == 1)) \n        # shape (number_of_occurences) by (lat_indices, lon_indices, time_indices)\n        self.neg_indices = np.column_stack(np.where(self.numpy_array[:,:,self.sequence_length:, -1] == 0))\n            \n        #ipdb.set_trace()\n        self.total_indices = None\n        \n        if (self.train_ratio): # True -> training; False -> eval\n            \n            neg_to_pos_ratio = 2\n            num_rows = len(self.pos_indices)*neg_to_pos_ratio\n            random_rows = np.random.choice(self.neg_indices.shape[0], size=num_rows, replace=False)\n            #ipdb.set_trace()\n            neg_indices_subset = self.neg_indices[random_rows, :]\n            #ipdb.set_trace()\n            \n            self.total_indices = np.concatenate((self.pos_indices, neg_indices_subset), axis=0)\n            #self.total_indices = np.concatenate((self.pos_indices, self.neg_indices), axis=0)\n            \n        else:\n            self.total_indices = np.concatenate((self.pos_indices, self.neg_indices), axis=0)\n        \n        np.random.shuffle(self.total_indices)\n        #ipdb.set_trace()\n            \n    def __len__(self): # num_samples * num_timesteps\n        return len(self.total_indices)\n\n    def __getitem__(self, index): # np array shape (40, 40, 918, 29), lat, lon, time, var\n        #total indices shape (num samples) by (lat_indices, lon_indices, time_indices)\n        lat_index = self.total_indices[index, 0]\n        lon_index = self.total_indices[index, 1]\n        time_index = self.total_indices[index, 2]\n\n        # ex timesteps 0 to 64: 65 timesteps in total, features takes 0 to 63, target is 64th timestep\n        target_np = self.numpy_array[lat_index, lon_index, time_index+sequence_length, -1] # only last variable gwis_ba\n        features_np = self.numpy_array[lat_index, lon_index, time_index:time_index+sequence_length, : ] # all variables\n        # lat, lon, time, var\n        \n        \n        return features_np.squeeze(), target_np.squeeze()\n\ndef temporal_dataloader(numpy_array, sequence_length, batch_size, train_ratio, pin_memory=False, num_workers=0):\n    temporal_dataset = TemporalDatasetFromNumpy(numpy_array, sequence_length, train_ratio, pin_memory)\n    dataloader = DataLoader(temporal_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=False)\n    return dataloader\n","metadata":{"id":"Dbga3drn8sCE","execution":{"iopub.status.busy":"2023-08-09T19:36:37.380836Z","iopub.status.idle":"2023-08-09T19:36:37.381134Z","shell.execute_reply.started":"2023-08-09T19:36:37.380991Z","shell.execute_reply":"2023-08-09T19:36:37.381008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainer + Evaluator","metadata":{"id":"zmt0TF_39e1Q"}},{"cell_type":"code","source":"# Define the training loop\ndef train(model, dataloader): #each dataloader gives 1203*150 samples\n    total_loss = 0.0\n    total_samples = 0.0\n\n    for batch_features, batch_target in tqdm(dataloader, desc = 'training'): #ndvi missing values\n        #ipdb.set_trace()\n        batch_features = batch_features.numpy()\n        batch_target = batch_target.numpy()\n        shape = batch_features.shape\n        batch_features_reshaped = batch_features.reshape(shape[0],-1)\n        #Drop all rows containing any nan:\n        #ipdb.set_trace()\n        mask = ~np.isnan(batch_features_reshaped).any(axis = 1)\n        batch_features_reshaped = batch_features_reshaped[mask]\n        #Reshape back:\n        #ipdb.set_trace()\n        batch_features = batch_features_reshaped#.reshape(batch_features_reshaped.shape[0],*shape[1:])\n        batch_target = batch_target[mask].squeeze()\n        #ipdb.set_trace()\n            \n        model.fit(batch_features, batch_target)\n        model.n_estimators += 1\n\ndef evaluate_model(model, dataloader):\n    \n    total_f1 = 0.0\n    total_recall = 0.0\n    total_precision = 0.0\n    total_accuracy = 0.0\n    total_aucroc = 0.0\n    total_auprc = 0.0\n    total_samples = 0.0 #because each batch is the same size\n\n    for inputs, targets in tqdm(dataloader, desc = 'evaluating'):\n        inputs = inputs.numpy()\n        targets = targets.numpy()\n        shape = inputs.shape\n        inputs_reshaped = inputs.reshape(shape[0],-1)\n        #Drop all rows containing any nan:\n        mask = ~np.isnan(inputs_reshaped).any(axis = 1)\n        inputs_reshaped = inputs_reshaped[mask]\n        #ipdb.set_trace()\n        inputs = inputs_reshaped#.reshape(inputs_reshaped.shape[0],*shape[1:])\n        targets = targets[mask].squeeze()\n\n        outputs = model.predict(inputs)\n        \n        num_samples = len(targets)\n\n        total_f1 += f1_score(targets, outputs, pos_label=1) * num_samples\n        total_precision += precision_score(targets, outputs, pos_label=1) * num_samples\n        total_accuracy += accuracy_score(targets, outputs) * num_samples\n        total_recall += recall_score(targets, outputs, pos_label=1) * num_samples\n        total_aucroc += roc_auc_score(targets, outputs) * num_samples\n        total_auprc += average_precision_score(targets, outputs, pos_label=1) * num_samples\n        total_samples += num_samples\n\n    #average_f1 = total_f1 / total_batches\n    #average_recall = total_recall / total_batches\n    #average_precision = total_precision / total_batches\n    #average_aucroc = total_aucroc / total_batches\n    #average_accuracy = total_accuracy / total_batches\n    return total_f1, total_recall, total_precision, total_auprc, total_aucroc, total_accuracy, total_samples","metadata":{"id":"Azn6vrTy9iyd","execution":{"iopub.status.busy":"2023-08-09T19:36:37.383125Z","iopub.status.idle":"2023-08-09T19:36:37.383646Z","shell.execute_reply.started":"2023-08-09T19:36:37.383455Z","shell.execute_reply":"2023-08-09T19:36:37.383471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RUN!!!","metadata":{"id":"3tJV3on_HTSa"}},{"cell_type":"code","source":"batch_size = 100\nfile_path = Path(\"/kaggle/input/california-spatial-temporal-fire-dataset/numpy_california_spatialtemporal_dataset.npy\")\nnumpy_array = np.load(file_path)[:,:,:700,:]\nmodel = RandomForestClassifier(n_estimators=1)\nsequence_length = 64\nfor i in range(10):\n    dataloader = temporal_dataloader(numpy_array, sequence_length, batch_size, train_ratio = True, pin_memory = False, num_workers=0)\n    train(model, dataloader)\nprint('done training')","metadata":{"execution":{"iopub.status.busy":"2023-08-09T19:36:37.384598Z","iopub.status.idle":"2023-08-09T19:36:37.385381Z","shell.execute_reply.started":"2023-08-09T19:36:37.385225Z","shell.execute_reply":"2023-08-09T19:36:37.385243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataloader = temporal_dataloader(np.load(file_path)[:,:,700:,:], sequence_length, batch_size, train_ratio = True, pin_memory = False, num_workers=0)\ntotal_f1, total_recall, total_precision, total_auprc, total_aucroc, total_accuracy, total_samples = evaluate_model(model, dataloader)\nprint(total_f1 / total_samples)\nprint(total_recall / total_samples)\nprint(total_precision / total_samples)\nprint(total_auprc / total_samples)\nprint(total_aucroc / total_samples)\nprint(total_accuracy / total_samples)\nprint(total_samples)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T19:36:37.386493Z","iopub.status.idle":"2023-08-09T19:36:37.387008Z","shell.execute_reply.started":"2023-08-09T19:36:37.386853Z","shell.execute_reply":"2023-08-09T19:36:37.386870Z"},"trusted":true},"execution_count":null,"outputs":[]}]}